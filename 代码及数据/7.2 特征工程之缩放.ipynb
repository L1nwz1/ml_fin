{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d38663a",
   "metadata": {},
   "source": [
    "## 7.3 数据放缩的必要性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c4ee13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a2d93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('california_housing.csv')\n",
    "feature_names = ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude']\n",
    "X_full = dataset[feature_names].values\n",
    "y_full = dataset['y'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf071a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c15ec6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['MedInc_amp'] = dataset['MedInc']*1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a701c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names_amp = ['MedInc_amp', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude']\n",
    "X_amp = dataset[feature_names_amp].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefd431d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['MedInc'].hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebd3cc5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset['MedInc_amp'].hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6678bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import median_absolute_error, r2_score\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.metrics import PredictionErrorDisplay\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_full, y_full, random_state=0)\n",
    "X_train_amp, X_test_amp, y_train_amp, y_test_amp = train_test_split(X_amp, y_full, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c4af0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import median_absolute_error, r2_score\n",
    "\n",
    "\n",
    "def compute_score(y_true, y_pred):\n",
    "    return {\n",
    "        \"R2\": f\"{r2_score(y_true, y_pred):.3f}\",\n",
    "        \"MedAE\": f\"{median_absolute_error(y_true, y_pred):.3f}\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40974f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ridge_cv = RidgeCV(alphas=(np.arange(500)+1)*0.5).fit(X_train, y_train)\n",
    "y_pred_ridge = ridge_cv.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "PredictionErrorDisplay.from_predictions(\n",
    "    y_test,\n",
    "    y_pred_ridge,\n",
    "    kind=\"actual_vs_predicted\",\n",
    "    #ax=ax0,\n",
    "    scatter_kwargs={\"alpha\": 0.5},\n",
    ")\n",
    "print(ridge_cv.alpha_)\n",
    "print(ridge_cv.coef_)\n",
    "compute_score(y_test,y_pred_ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1692ea3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_cv_amp = RidgeCV((np.arange(500)+1)*0.5).fit(X_train_amp, y_train_amp)\n",
    "y_pred_ridge_amp = ridge_cv_amp.predict(X_test_amp)\n",
    "\n",
    "PredictionErrorDisplay.from_predictions(\n",
    "    y_test_amp,\n",
    "    y_pred_ridge_amp,\n",
    "    kind=\"actual_vs_predicted\",\n",
    "    #ax=ax0,\n",
    "    scatter_kwargs={\"alpha\": 0.5},\n",
    ")\n",
    "print(ridge_cv_amp.alpha_)\n",
    "print(ridge_cv_amp.coef_)\n",
    "compute_score(y_test_amp,y_pred_ridge_amp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33dea133",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "\n",
    "ridge_cv_with_trans_target = TransformedTargetRegressor(\n",
    "    regressor=RidgeCV((np.arange(500)+1)*0.5),\n",
    "    transformer=QuantileTransformer(n_quantiles=900, output_distribution=\"normal\"),\n",
    ").fit(X_train, y_train)\n",
    "y_pred_ridge_with_trans_target = ridge_cv_with_trans_target.predict(X_test)\n",
    "\n",
    "PredictionErrorDisplay.from_predictions(\n",
    "    y_test,\n",
    "    y_pred_ridge_with_trans_target,\n",
    "    kind=\"actual_vs_predicted\",\n",
    "    #ax=ax0,\n",
    "    scatter_kwargs={\"alpha\": 0.5},\n",
    ")\n",
    "#print(ridge_cv_with_trans_target.alpha_)\n",
    "#print(ridge_cv_with_trans_target.coef_)\n",
    "compute_score(y_test,y_pred_ridge_with_trans_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a8309b",
   "metadata": {},
   "source": [
    "## 7.4 真实数据的多种放缩"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5acdc6f",
   "metadata": {},
   "source": [
    "本例改编自 https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py\n",
    "\n",
    "在本例中展示了几种主要的放缩方法，在应对带有异常值数据中的表现\n",
    "\n",
    "代码修改数据读取方式为本地以在一般网络条件下运行，请不要二次传播数据\n",
    "\n",
    "请注意中文注释"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a75c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author:  Raghav RV <rvraghav93@gmail.com>\n",
    "#          Guillaume Lemaitre <g.lemaitre58@gmail.com>\n",
    "#          Thomas Unterthiner\n",
    "# License: BSD 3 clause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d845c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "from matplotlib import cm\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "#依次调用了8种数据变换模式\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import (\n",
    "    MaxAbsScaler,\n",
    "    MinMaxScaler,\n",
    "    Normalizer,\n",
    "    PowerTransformer,\n",
    "    QuantileTransformer,\n",
    "    RobustScaler,\n",
    "    StandardScaler,\n",
    "    minmax_scale,\n",
    ")\n",
    "\n",
    "#dataset = fetch_california_housing()\n",
    "\n",
    "#X_full, y_full = dataset.data, dataset.target\n",
    "#线上读取数据，可能因为网络问题不能施行，故换线下读取\n",
    "\n",
    "dataset = pd.read_csv('california_housing.csv')\n",
    "feature_names = ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude']\n",
    "X_full = dataset[feature_names].values\n",
    "y_full = dataset['y'].values\n",
    "feature_mapping = {\n",
    "    \"MedInc\": \"Median income in block in $1,000\",\n",
    "    \"HouseAge\": \"Median house age in block\",\n",
    "    \"AveRooms\": \"Average number of rooms\",\n",
    "    \"AveBedrms\": \"Average number of bedrooms\",\n",
    "    \"Population\": \"Block population\",\n",
    "    \"AveOccup\": \"Average house occupancy\",\n",
    "    \"Latitude\": \"House block latitude\",\n",
    "    \"Longitude\": \"House block longitude\",\n",
    "    \"y\":\"Median House Price in $100,000\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca29a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005c7bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a897e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#保存线上数据，不需要运行\n",
    "# var_dic = {}\n",
    "# for xi in range(len(X_full[0])):\n",
    "#     var_dic[feature_names[xi]] = X_full[:,xi]\n",
    "# var_dic['y'] = y_full\n",
    "# data_df = pd.DataFrame(var_dic)\n",
    "# data_df.to_csv('california_housing.csv',index=None,encoding='utf-8_sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86129e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take only 2 features to make visualization easier\n",
    "# Feature MedInc has a long tail distribution 厚尾分布，即斜度>0.\n",
    "# Feature AveOccup has a few but very large outliers. 有严重异常值\n",
    "features = [\"MedInc\", \"AveOccup\"]\n",
    "features_idx = [feature_names.index(feature) for feature in features]\n",
    "X = X_full[:, features_idx]\n",
    "distributions = [\n",
    "    (\"Unscaled data\", X),\n",
    "    (\"Data after standard scaling\", StandardScaler().fit_transform(X)),\n",
    "    (\"Data after min-max scaling\", MinMaxScaler().fit_transform(X)),\n",
    "    (\"Data after max-abs scaling\", MaxAbsScaler().fit_transform(X)),\n",
    "    (\n",
    "        \"Data after robust scaling\",\n",
    "        #注意这里上下限分位数的限制\n",
    "        RobustScaler(quantile_range=(25, 75)).fit_transform(X),\n",
    "    ),\n",
    "    (\n",
    "        \"Data after power transformation (Yeo-Johnson)\",\n",
    "        PowerTransformer(method=\"yeo-johnson\").fit_transform(X),\n",
    "    ),\n",
    "    (\n",
    "        \"Data after power transformation (Box-Cox)\",\n",
    "        PowerTransformer(method=\"box-cox\").fit_transform(X),\n",
    "    ),\n",
    "    (\n",
    "        \"Data after quantile transformation (uniform pdf)\",\n",
    "        QuantileTransformer(\n",
    "            output_distribution=\"uniform\", random_state=42\n",
    "        ).fit_transform(X),\n",
    "    ),\n",
    "    (\n",
    "        \"Data after quantile transformation (gaussian pdf)\",\n",
    "        QuantileTransformer(\n",
    "            output_distribution=\"normal\", random_state=42\n",
    "        ).fit_transform(X),\n",
    "    ),\n",
    "    (\"Data after sample-wise L2 normalizing\", Normalizer().fit_transform(X)),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9cee15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the output between 0 and 1 for the colorbar\n",
    "y = minmax_scale(y_full)\n",
    "\n",
    "# plasma does not exist in matplotlib < 1.5\n",
    "cmap = getattr(cm, \"plasma_r\", cm.hot_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9abd2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#绘图代码十分精致，但没有必要细究\n",
    "#重点掌握其实现：左图所有数据，右图只看[1%,99%]的数据，即排除了异常值\n",
    "#在横纵轴展示了两种数据的直方图分布\n",
    "def create_axes(title, figsize=(16, 6)):\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    fig.suptitle(title)\n",
    "\n",
    "    # define the axis for the first plot\n",
    "    left, width = 0.1, 0.22\n",
    "    bottom, height = 0.1, 0.7\n",
    "    bottom_h = height + 0.15\n",
    "    left_h = left + width + 0.02\n",
    "\n",
    "    rect_scatter = [left, bottom, width, height]\n",
    "    rect_histx = [left, bottom_h, width, 0.1]\n",
    "    rect_histy = [left_h, bottom, 0.05, height]\n",
    "\n",
    "    ax_scatter = plt.axes(rect_scatter)\n",
    "    ax_histx = plt.axes(rect_histx)\n",
    "    ax_histy = plt.axes(rect_histy)\n",
    "\n",
    "    # define the axis for the zoomed-in plot\n",
    "    left = width + left + 0.2\n",
    "    left_h = left + width + 0.02\n",
    "\n",
    "    rect_scatter = [left, bottom, width, height]\n",
    "    rect_histx = [left, bottom_h, width, 0.1]\n",
    "    rect_histy = [left_h, bottom, 0.05, height]\n",
    "\n",
    "    ax_scatter_zoom = plt.axes(rect_scatter)\n",
    "    ax_histx_zoom = plt.axes(rect_histx)\n",
    "    ax_histy_zoom = plt.axes(rect_histy)\n",
    "\n",
    "    # define the axis for the colorbar\n",
    "    left, width = width + left + 0.13, 0.01\n",
    "\n",
    "    rect_colorbar = [left, bottom, width, height]\n",
    "    ax_colorbar = plt.axes(rect_colorbar)\n",
    "\n",
    "    return (\n",
    "        (ax_scatter, ax_histy, ax_histx),\n",
    "        (ax_scatter_zoom, ax_histy_zoom, ax_histx_zoom),\n",
    "        ax_colorbar,\n",
    "    )\n",
    "\n",
    "\n",
    "def plot_distribution(axes, X, y, hist_nbins=50, title=\"\", x0_label=\"\", x1_label=\"\"):\n",
    "    ax, hist_X1, hist_X0 = axes\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(x0_label)\n",
    "    ax.set_ylabel(x1_label)\n",
    "\n",
    "    # The scatter plot\n",
    "    colors = cmap(y)\n",
    "    ax.scatter(X[:, 0], X[:, 1], alpha=0.5, marker=\"o\", s=5, lw=0, c=colors)\n",
    "\n",
    "    # Removing the top and the right spine for aesthetics\n",
    "    # make nice axis layout\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.get_xaxis().tick_bottom()\n",
    "    ax.get_yaxis().tick_left()\n",
    "    ax.spines[\"left\"].set_position((\"outward\", 10))\n",
    "    ax.spines[\"bottom\"].set_position((\"outward\", 10))\n",
    "\n",
    "    # Histogram for axis X1 (feature 5)\n",
    "    hist_X1.set_ylim(ax.get_ylim())\n",
    "    hist_X1.hist(\n",
    "        X[:, 1], bins=hist_nbins, orientation=\"horizontal\", color=\"grey\", ec=\"grey\"\n",
    "    )\n",
    "    hist_X1.axis(\"off\")\n",
    "\n",
    "    # Histogram for axis X0 (feature 0)\n",
    "    hist_X0.set_xlim(ax.get_xlim())\n",
    "    hist_X0.hist(\n",
    "        X[:, 0], bins=hist_nbins, orientation=\"vertical\", color=\"grey\", ec=\"grey\"\n",
    "    )\n",
    "    hist_X0.axis(\"off\")\n",
    "    \n",
    "def make_plot(item_idx):\n",
    "    title, X = distributions[item_idx]\n",
    "    ax_zoom_out, ax_zoom_in, ax_colorbar = create_axes(title)\n",
    "    axarr = (ax_zoom_out, ax_zoom_in)\n",
    "    plot_distribution(\n",
    "        axarr[0],\n",
    "        X,\n",
    "        y,\n",
    "        hist_nbins=200,\n",
    "        x0_label=feature_mapping[features[0]],\n",
    "        x1_label=feature_mapping[features[1]],\n",
    "        title=\"Full data\",\n",
    "    )\n",
    "\n",
    "    # zoom-in\n",
    "    zoom_in_percentile_range = (0, 99)\n",
    "    cutoffs_X0 = np.percentile(X[:, 0], zoom_in_percentile_range)\n",
    "    cutoffs_X1 = np.percentile(X[:, 1], zoom_in_percentile_range)\n",
    "\n",
    "    non_outliers_mask = np.all(X > [cutoffs_X0[0], cutoffs_X1[0]], axis=1) & np.all(\n",
    "        X < [cutoffs_X0[1], cutoffs_X1[1]], axis=1\n",
    "    )\n",
    "    plot_distribution(\n",
    "        axarr[1],\n",
    "        X[non_outliers_mask],\n",
    "        y[non_outliers_mask],\n",
    "        hist_nbins=50,\n",
    "        x0_label=feature_mapping[features[0]],\n",
    "        x1_label=feature_mapping[features[1]],\n",
    "        title=\"Zoom-in\",\n",
    "    )\n",
    "\n",
    "    norm = mpl.colors.Normalize(y_full.min(), y_full.max())\n",
    "    mpl.colorbar.ColorbarBase(\n",
    "        ax_colorbar,\n",
    "        cmap=cmap,\n",
    "        norm=norm,\n",
    "        orientation=\"vertical\",\n",
    "        label=\"Color mapping for values of y\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ffb13c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "44fa1128",
   "metadata": {},
   "source": [
    "## 7.4.1 原始数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5facd18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_plot(0)\n",
    "#原始数据，左图是全部，右图是进行[1%,99%]缩尾处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7dfef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "xdata = pd.DataFrame({\"MedInc\":X[:,0], \"AveOccup\":X[:,1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd556a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "xdata.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c6e037",
   "metadata": {},
   "outputs": [],
   "source": [
    "#收入中位数分布图\n",
    "xdata['MedInc'].hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37f1ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(xdata['MedInc']).hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fa389f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406d1fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#平均房屋入住率存在异常值，使得分布很奇怪\n",
    "xdata['AveOccup'].hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c784307",
   "metadata": {},
   "outputs": [],
   "source": [
    "xdata['AveOccup'].quantile(0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de65c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#排除1%的点后\n",
    "xdata[xdata['AveOccup']<=xdata['AveOccup'].quantile(0.99)]['AveOccup'].hist(bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab38b465",
   "metadata": {},
   "source": [
    "## 7.4.2 标准化\n",
    "使用 StandardScaler 详细链接：https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler \n",
    "\n",
    "将数据压缩至N(0,1)，但是无法应对极端离群值，\n",
    "\n",
    "压缩方法非常简单粗暴：value_scaled = (value-mean)/standard_error\n",
    "\n",
    "这个适用于数据本身分布具有一些正态分布的影子且没有（或已预先去除）极端值\n",
    "\n",
    "标准化可以使得不同feature的方差统一，使得L1、L2、高斯核等方法可以应用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a917768a",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_plot(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef1f438",
   "metadata": {},
   "source": [
    "## 7.4.3 最大最小放缩\n",
    "使用 MinMaxScaler 详见 https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler\n",
    "将数据压缩到[0,1]的范围内，但仍然无法处理异常值\n",
    "注意纵轴数据的取值范围\n",
    "\n",
    "稀疏矩阵处理——保证稀疏，保证0。想一想之前讲过的例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d05d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_plot(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e27b3f",
   "metadata": {},
   "source": [
    "## 7.4.4 最大绝对值放缩\n",
    "使用 MaxAbsScaler https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MaxAbsScaler.html#sklearn.preprocessing.MaxAbsScaler\n",
    "类似最大最小放缩，但会根据数据进行选择\n",
    "如果都是正数，则放缩至[0,1]\n",
    "如果都是负数，则放缩至[-1,0]\n",
    "如果有正数有负数，则放缩至[-1,1]\n",
    "\n",
    "与MinMaxScaler类似，这种方法仍然无助于解决极端值的问题。\n",
    "在本案例中，MaxAbs与MinMax产生的结果相同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d502ad73",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_plot(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e57c27",
   "metadata": {},
   "source": [
    "## 7.4.5 稳健放缩\n",
    "使用RobustScaler，参见 https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html#sklearn.preprocessing.RobustScaler\n",
    "放缩公式为 scaled_value = (value - median)/(q75-q25)\n",
    "其中q25、q75为分位数，默认值为25,75，可以在函数中更改\n",
    "\n",
    "相较于之前的放缩器，稳健放缩的放缩值分散更广，但是仍然无法解决异常值的问题\n",
    "（中位数、分位数本就是具有较高的应对异常值的能力，即放缩器不能特异性地处理异常）\n",
    "\n",
    "稳健缩放的“稳健”体现在，当数据包含进一些极端值时，放缩的处理方式因为不依赖 于最大最小值、均值，所以放缩器不发生变化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77866c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_plot(4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3b5f63",
   "metadata": {},
   "source": [
    "## 7.4.6 幂变换\n",
    "PowerTransformer 相加https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PowerTransformer.html#sklearn.preprocessing.PowerTransformer\n",
    "非线性变化，使数据趋向于N(0,1)的高斯分布同时最小化偏度（skewness,三阶矩，skewness绝对值小则分布更正，不出现厚尾薄尾的情况）\n",
    "\n",
    "幂变换是非线性变化，即保证了 x > y 与 f(x) > f(y) 的等价\n",
    "\n",
    "但无法保证 f(x-y) = f(x) - f(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c122c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_plot(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c6276d",
   "metadata": {},
   "source": [
    "### Yeo-Johnson 和 Box-Cox\n",
    "sklearn现在支持Yeo-Johnson方法（上图）和Box-Cox方法（下图）\n",
    "后者只能支持正数，前者可以支持存在负数的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d6cf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_plot(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa910c5",
   "metadata": {},
   "source": [
    "## 7.4.7 分位数变换\n",
    "非线性变换，强行让数据变形为均匀分布或者高斯分布\n",
    "\n",
    "这种变换非常强力，不论数据原本的形态为何（可以参见下文）\n",
    "\n",
    "同时，由于过于强力，在样本点不足够多（小于1000个）的情况下，不要使用\n",
    "\n",
    "对于异常值，有一定的稳健性（边界预先设定且不随异常数据变化）但是极端值变化后可能发生过曝（1000和10000的点因为过大都被映射成最大值而没有区别）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368043bc",
   "metadata": {},
   "source": [
    "### 分位数变换：均匀分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b508c511",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_plot(7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee01ab8",
   "metadata": {},
   "source": [
    "### 分位数变换：高斯分布输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e57116",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_plot(8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1f26cf",
   "metadata": {},
   "source": [
    "# 7.5 幂变换 vs 分位数变换\n",
    "下面的代码，展示了在原有分布为 \n",
    "\n",
    "对数分布、卡方分布、韦伯分布\n",
    "\n",
    "高斯分布、均匀分布、双峰分布\n",
    "\n",
    "条件下，box-cox、yeo-johnson、分位数变换的结果\n",
    "\n",
    "三组代码分别显示了1000个、100个、10000个数据点的结果\n",
    "\n",
    "请注意不同原分布、样本量上三种转换的表现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a56133",
   "metadata": {},
   "source": [
    "本案例改编自 https://scikit-learn.org/stable/auto_examples/preprocessing/plot_map_data_to_normal.html#sphx-glr-auto-examples-preprocessing-plot-map-data-to-normal-py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2efd16",
   "metadata": {},
   "source": [
    "## 7.5.1 1000样本点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a45965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Eric Chang <ericchang2017@u.northwestern.edu>\n",
    "#         Nicolas Hug <contact@nicolas-hug.com>\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PowerTransformer, QuantileTransformer\n",
    "\n",
    "N_SAMPLES = 1000\n",
    "FONT_SIZE = 24\n",
    "BINS = 30\n",
    "\n",
    "\n",
    "rng = np.random.RandomState(304)\n",
    "bc = PowerTransformer(method=\"box-cox\")\n",
    "yj = PowerTransformer(method=\"yeo-johnson\")\n",
    "# n_quantiles is set to the training set size rather than the default value\n",
    "# to avoid a warning being raised by this example\n",
    "qt = QuantileTransformer(\n",
    "    n_quantiles=500, output_distribution=\"normal\", random_state=rng\n",
    ")\n",
    "size = (N_SAMPLES, 1)\n",
    "\n",
    "\n",
    "# lognormal distribution\n",
    "X_lognormal = rng.lognormal(size=size)\n",
    "\n",
    "# chi-squared distribution\n",
    "df = 3\n",
    "X_chisq = rng.chisquare(df=df, size=size)\n",
    "\n",
    "# weibull distribution\n",
    "a = 50\n",
    "X_weibull = rng.weibull(a=a, size=size)\n",
    "\n",
    "# gaussian distribution\n",
    "loc = 100\n",
    "X_gaussian = rng.normal(loc=loc, size=size)\n",
    "\n",
    "# uniform distribution\n",
    "X_uniform = rng.uniform(low=0, high=1, size=size)\n",
    "\n",
    "# bimodal distribution\n",
    "loc_a, loc_b = 100, 105\n",
    "X_a, X_b = rng.normal(loc=loc_a, size=size), rng.normal(loc=loc_b, size=size)\n",
    "X_bimodal = np.concatenate([X_a, X_b], axis=0)\n",
    "\n",
    "\n",
    "# create plots\n",
    "distributions = [\n",
    "    (\"Lognormal\", X_lognormal),\n",
    "    (\"Chi-squared\", X_chisq),\n",
    "    (\"Weibull\", X_weibull),\n",
    "    (\"Gaussian\", X_gaussian),\n",
    "    (\"Uniform\", X_uniform),\n",
    "    (\"Bimodal\", X_bimodal),\n",
    "]\n",
    "\n",
    "colors = [\"#D81B60\", \"#0188FF\", \"#FFC107\", \"#B7A2FF\", \"#000000\", \"#2EC5AC\"]\n",
    "\n",
    "fig, axes = plt.subplots(nrows=8, ncols=3, figsize=(16,36))\n",
    "axes = axes.flatten()\n",
    "axes_idxs = [\n",
    "    (0, 3, 6, 9),\n",
    "    (1, 4, 7, 10),\n",
    "    (2, 5, 8, 11),\n",
    "    (12, 15, 18, 21),\n",
    "    (13, 16, 19, 22),\n",
    "    (14, 17, 20, 23),\n",
    "]\n",
    "axes_list = [(axes[i], axes[j], axes[k], axes[l]) for (i, j, k, l) in axes_idxs]\n",
    "\n",
    "\n",
    "for distribution, color, axes in zip(distributions, colors, axes_list):\n",
    "    name, X = distribution\n",
    "    X_train, X_test = train_test_split(X, test_size=0.5)\n",
    "\n",
    "    # perform power transforms and quantile transform\n",
    "    X_trans_bc = bc.fit(X_train).transform(X_test)\n",
    "    lmbda_bc = round(bc.lambdas_[0], 2)\n",
    "    X_trans_yj = yj.fit(X_train).transform(X_test)\n",
    "    lmbda_yj = round(yj.lambdas_[0], 2)\n",
    "    X_trans_qt = qt.fit(X_train).transform(X_test)\n",
    "\n",
    "    ax_original, ax_bc, ax_yj, ax_qt = axes\n",
    "\n",
    "    ax_original.hist(X_train, color=color, bins=BINS)\n",
    "    ax_original.set_title(name, fontsize=FONT_SIZE)\n",
    "    ax_original.tick_params(axis=\"both\", which=\"major\", labelsize=FONT_SIZE)\n",
    "\n",
    "    for ax, X_trans, meth_name, lmbda in zip(\n",
    "        (ax_bc, ax_yj, ax_qt),\n",
    "        (X_trans_bc, X_trans_yj, X_trans_qt),\n",
    "        (\"Box-Cox\", \"Yeo-Johnson\", \"Quantile transform\"),\n",
    "        (lmbda_bc, lmbda_yj, None),\n",
    "    ):\n",
    "        ax.hist(X_trans, color=color, bins=BINS)\n",
    "        title = \"After {}\".format(meth_name)\n",
    "        if lmbda is not None:\n",
    "            title += \"\\n$\\\\lambda$ = {}\".format(lmbda)\n",
    "        ax.set_title(title, fontsize=FONT_SIZE)\n",
    "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=FONT_SIZE)\n",
    "        ax.set_xlim([-3.5, 3.5])\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('nsample_1000')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6bf2ab",
   "metadata": {},
   "source": [
    "## 1000样本点结果解析\n",
    "请注意，在对数分布、卡方分布的原数据上，box-cox取得了较好的结果，但是请注意这种方法只能处理正数数据\n",
    "\n",
    "在均匀分布、双峰分布上，幂变换的效果都很糟糕"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16cd228",
   "metadata": {},
   "source": [
    "## 7.5.2 100个样本点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5037e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "N_SAMPLES = 100\n",
    "FONT_SIZE = 24\n",
    "BINS = 30\n",
    "\n",
    "\n",
    "rng = np.random.RandomState(304)\n",
    "bc = PowerTransformer(method=\"box-cox\")\n",
    "yj = PowerTransformer(method=\"yeo-johnson\")\n",
    "# n_quantiles is set to the training set size rather than the default value\n",
    "# to avoid a warning being raised by this example\n",
    "qt = QuantileTransformer(\n",
    "    n_quantiles=50, output_distribution=\"normal\", random_state=rng\n",
    ")\n",
    "size = (N_SAMPLES, 1)\n",
    "\n",
    "\n",
    "# lognormal distribution\n",
    "X_lognormal = rng.lognormal(size=size)\n",
    "\n",
    "# chi-squared distribution\n",
    "df = 3\n",
    "X_chisq = rng.chisquare(df=df, size=size)\n",
    "\n",
    "# weibull distribution\n",
    "a = 50\n",
    "X_weibull = rng.weibull(a=a, size=size)\n",
    "\n",
    "# gaussian distribution\n",
    "loc = 100\n",
    "X_gaussian = rng.normal(loc=loc, size=size)\n",
    "\n",
    "# uniform distribution\n",
    "X_uniform = rng.uniform(low=0, high=1, size=size)\n",
    "\n",
    "# bimodal distribution\n",
    "loc_a, loc_b = 100, 105\n",
    "X_a, X_b = rng.normal(loc=loc_a, size=size), rng.normal(loc=loc_b, size=size)\n",
    "X_bimodal = np.concatenate([X_a, X_b], axis=0)\n",
    "\n",
    "\n",
    "# create plots\n",
    "distributions = [\n",
    "    (\"Lognormal\", X_lognormal),\n",
    "    (\"Chi-squared\", X_chisq),\n",
    "    (\"Weibull\", X_weibull),\n",
    "    (\"Gaussian\", X_gaussian),\n",
    "    (\"Uniform\", X_uniform),\n",
    "    (\"Bimodal\", X_bimodal),\n",
    "]\n",
    "\n",
    "colors = [\"#D81B60\", \"#0188FF\", \"#FFC107\", \"#B7A2FF\", \"#000000\", \"#2EC5AC\"]\n",
    "\n",
    "fig, axes = plt.subplots(nrows=8, ncols=3, figsize=(16,36))\n",
    "axes = axes.flatten()\n",
    "axes_idxs = [\n",
    "    (0, 3, 6, 9),\n",
    "    (1, 4, 7, 10),\n",
    "    (2, 5, 8, 11),\n",
    "    (12, 15, 18, 21),\n",
    "    (13, 16, 19, 22),\n",
    "    (14, 17, 20, 23),\n",
    "]\n",
    "axes_list = [(axes[i], axes[j], axes[k], axes[l]) for (i, j, k, l) in axes_idxs]\n",
    "\n",
    "\n",
    "for distribution, color, axes in zip(distributions, colors, axes_list):\n",
    "    name, X = distribution\n",
    "    X_train, X_test = train_test_split(X, test_size=0.5)\n",
    "\n",
    "    # perform power transforms and quantile transform\n",
    "    X_trans_bc = bc.fit(X_train).transform(X_test)\n",
    "    lmbda_bc = round(bc.lambdas_[0], 2)\n",
    "    X_trans_yj = yj.fit(X_train).transform(X_test)\n",
    "    lmbda_yj = round(yj.lambdas_[0], 2)\n",
    "    X_trans_qt = qt.fit(X_train).transform(X_test)\n",
    "\n",
    "    ax_original, ax_bc, ax_yj, ax_qt = axes\n",
    "\n",
    "    ax_original.hist(X_train, color=color, bins=BINS)\n",
    "    ax_original.set_title(name, fontsize=FONT_SIZE)\n",
    "    ax_original.tick_params(axis=\"both\", which=\"major\", labelsize=FONT_SIZE)\n",
    "\n",
    "    for ax, X_trans, meth_name, lmbda in zip(\n",
    "        (ax_bc, ax_yj, ax_qt),\n",
    "        (X_trans_bc, X_trans_yj, X_trans_qt),\n",
    "        (\"Box-Cox\", \"Yeo-Johnson\", \"Quantile transform\"),\n",
    "        (lmbda_bc, lmbda_yj, None),\n",
    "    ):\n",
    "        ax.hist(X_trans, color=color, bins=BINS)\n",
    "        title = \"After {}\".format(meth_name)\n",
    "        if lmbda is not None:\n",
    "            title += \"\\n$\\\\lambda$ = {}\".format(lmbda)\n",
    "        ax.set_title(title, fontsize=FONT_SIZE)\n",
    "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=FONT_SIZE)\n",
    "        ax.set_xlim([-3.5, 3.5])\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('nsample_100')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9184d1f7",
   "metadata": {},
   "source": [
    "## 100样本点数据解析\n",
    "当样本点数量较少时，变换的效果明显下降，分位数变化出现了更为明显的过拟合问题"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed4cf32",
   "metadata": {},
   "source": [
    "## 7.5.3 10000个样本点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ae3364",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "N_SAMPLES = 10000\n",
    "FONT_SIZE = 24\n",
    "BINS = 30\n",
    "\n",
    "\n",
    "rng = np.random.RandomState(304)\n",
    "bc = PowerTransformer(method=\"box-cox\")\n",
    "yj = PowerTransformer(method=\"yeo-johnson\")\n",
    "# n_quantiles is set to the training set size rather than the default value\n",
    "# to avoid a warning being raised by this example\n",
    "qt = QuantileTransformer(\n",
    "    n_quantiles=5000, output_distribution=\"normal\", random_state=rng\n",
    ")\n",
    "size = (N_SAMPLES, 1)\n",
    "\n",
    "\n",
    "# lognormal distribution\n",
    "X_lognormal = rng.lognormal(size=size)\n",
    "\n",
    "# chi-squared distribution\n",
    "df = 3\n",
    "X_chisq = rng.chisquare(df=df, size=size)\n",
    "\n",
    "# weibull distribution\n",
    "a = 50\n",
    "X_weibull = rng.weibull(a=a, size=size)\n",
    "\n",
    "# gaussian distribution\n",
    "loc = 100\n",
    "X_gaussian = rng.normal(loc=loc, size=size)\n",
    "\n",
    "# uniform distribution\n",
    "X_uniform = rng.uniform(low=0, high=1, size=size)\n",
    "\n",
    "# bimodal distribution\n",
    "loc_a, loc_b = 100, 105\n",
    "X_a, X_b = rng.normal(loc=loc_a, size=size), rng.normal(loc=loc_b, size=size)\n",
    "X_bimodal = np.concatenate([X_a, X_b], axis=0)\n",
    "\n",
    "\n",
    "# create plots\n",
    "distributions = [\n",
    "    (\"Lognormal\", X_lognormal),\n",
    "    (\"Chi-squared\", X_chisq),\n",
    "    (\"Weibull\", X_weibull),\n",
    "    (\"Gaussian\", X_gaussian),\n",
    "    (\"Uniform\", X_uniform),\n",
    "    (\"Bimodal\", X_bimodal),\n",
    "]\n",
    "\n",
    "colors = [\"#D81B60\", \"#0188FF\", \"#FFC107\", \"#B7A2FF\", \"#000000\", \"#2EC5AC\"]\n",
    "\n",
    "fig, axes = plt.subplots(nrows=8, ncols=3, figsize=(16,36))\n",
    "axes = axes.flatten()\n",
    "axes_idxs = [\n",
    "    (0, 3, 6, 9),\n",
    "    (1, 4, 7, 10),\n",
    "    (2, 5, 8, 11),\n",
    "    (12, 15, 18, 21),\n",
    "    (13, 16, 19, 22),\n",
    "    (14, 17, 20, 23),\n",
    "]\n",
    "axes_list = [(axes[i], axes[j], axes[k], axes[l]) for (i, j, k, l) in axes_idxs]\n",
    "\n",
    "\n",
    "for distribution, color, axes in zip(distributions, colors, axes_list):\n",
    "    name, X = distribution\n",
    "    X_train, X_test = train_test_split(X, test_size=0.5)\n",
    "\n",
    "    # perform power transforms and quantile transform\n",
    "    X_trans_bc = bc.fit(X_train).transform(X_test)\n",
    "    lmbda_bc = round(bc.lambdas_[0], 2)\n",
    "    X_trans_yj = yj.fit(X_train).transform(X_test)\n",
    "    lmbda_yj = round(yj.lambdas_[0], 2)\n",
    "    X_trans_qt = qt.fit(X_train).transform(X_test)\n",
    "\n",
    "    ax_original, ax_bc, ax_yj, ax_qt = axes\n",
    "\n",
    "    ax_original.hist(X_train, color=color, bins=BINS)\n",
    "    ax_original.set_title(name, fontsize=FONT_SIZE)\n",
    "    ax_original.tick_params(axis=\"both\", which=\"major\", labelsize=FONT_SIZE)\n",
    "\n",
    "    for ax, X_trans, meth_name, lmbda in zip(\n",
    "        (ax_bc, ax_yj, ax_qt),\n",
    "        (X_trans_bc, X_trans_yj, X_trans_qt),\n",
    "        (\"Box-Cox\", \"Yeo-Johnson\", \"Quantile transform\"),\n",
    "        (lmbda_bc, lmbda_yj, None),\n",
    "    ):\n",
    "        ax.hist(X_trans, color=color, bins=BINS)\n",
    "        title = \"After {}\".format(meth_name)\n",
    "        if lmbda is not None:\n",
    "            title += \"\\n$\\\\lambda$ = {}\".format(lmbda)\n",
    "        ax.set_title(title, fontsize=FONT_SIZE)\n",
    "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=FONT_SIZE)\n",
    "        ax.set_xlim([-3.5, 3.5])\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('nsample_10000')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157528bd",
   "metadata": {},
   "source": [
    "## 10000样本点解析\n",
    "样本点数量多后，数据形态与结果间差别更为明显"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc08ec9",
   "metadata": {},
   "source": [
    "# 7.6 数据离散化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdf456a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Andreas Müller\n",
    "#         Hanmin Qin <qinhanmin2005@sina.com>\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# construct the dataset\n",
    "rnd = np.random.RandomState(42)\n",
    "X = rnd.uniform(-3, 3, size=100)\n",
    "y = np.sin(X) + rnd.normal(size=len(X)) / 3\n",
    "X = X.reshape(-1, 1)\n",
    "\n",
    "# transform the dataset with KBinsDiscretizer\n",
    "#这两句为变形代码，分为十个bin，同时使用onehot代码\n",
    "enc = KBinsDiscretizer(n_bins=10, encode=\"onehot\")\n",
    "X_binned = enc.fit_transform(X)\n",
    "\n",
    "# predict with original dataset\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True, figsize=(10, 4))\n",
    "line = np.linspace(-3, 3, 1000, endpoint=False).reshape(-1, 1)\n",
    "reg = LinearRegression().fit(X, y)\n",
    "ax1.plot(line, reg.predict(line), linewidth=2, color=\"green\", label=\"linear regression\")\n",
    "reg = DecisionTreeRegressor(min_samples_split=3, random_state=0).fit(X, y)\n",
    "ax1.plot(line, reg.predict(line), linewidth=2, color=\"red\", label=\"decision tree\")\n",
    "ax1.plot(X[:, 0], y, \"o\", c=\"k\")\n",
    "ax1.legend(loc=\"best\")\n",
    "ax1.set_ylabel(\"Regression output\")\n",
    "ax1.set_xlabel(\"Input feature\")\n",
    "ax1.set_title(\"Result before discretization\")\n",
    "\n",
    "# predict with transformed dataset\n",
    "line_binned = enc.transform(line)\n",
    "reg = LinearRegression().fit(X_binned, y)\n",
    "ax2.plot(\n",
    "    line,\n",
    "    reg.predict(line_binned),\n",
    "    linewidth=2,\n",
    "    color=\"green\",\n",
    "    linestyle=\"-\",\n",
    "    label=\"linear regression\",\n",
    ")\n",
    "reg = DecisionTreeRegressor(min_samples_split=3, random_state=0).fit(X_binned, y)\n",
    "ax2.plot(\n",
    "    line,\n",
    "    reg.predict(line_binned),\n",
    "    linewidth=2,\n",
    "    color=\"red\",\n",
    "    linestyle=\":\",\n",
    "    label=\"decision tree\",\n",
    ")\n",
    "ax2.plot(X[:, 0], y, \"o\", c=\"k\")\n",
    "ax2.vlines(enc.bin_edges_[0], *plt.gca().get_ylim(), linewidth=1, alpha=0.2)\n",
    "ax2.legend(loc=\"best\")\n",
    "ax2.set_xlabel(\"Input feature\")\n",
    "ax2.set_title(\"Result after discretization\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e2176c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code source: Tom Dupré la Tour\n",
    "# Adapted from plot_classifier_comparison by Gaël Varoquaux and Andreas Müller\n",
    "#\n",
    "# License: BSD 3 clause\n",
    "\n",
    "#这段代码是一个学习pipeline的好样例\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "from sklearn.datasets import make_circles, make_classification, make_moons\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import KBinsDiscretizer, StandardScaler\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.utils._testing import ignore_warnings\n",
    "\n",
    "h = 0.02  # step size in the mesh\n",
    "\n",
    "\n",
    "def get_name(estimator):\n",
    "    name = estimator.__class__.__name__\n",
    "    if name == \"Pipeline\":\n",
    "        name = [get_name(est[1]) for est in estimator.steps]\n",
    "        name = \" + \".join(name)\n",
    "    return name\n",
    "\n",
    "\n",
    "# list of (estimator, param_grid), where param_grid is used in GridSearchCV\n",
    "# The parameter spaces in this example are limited to a narrow band to reduce\n",
    "# its runtime. In a real use case, a broader search space for the algorithms\n",
    "# should be used.\n",
    "classifiers = [\n",
    "    (\n",
    "        make_pipeline(StandardScaler(), LogisticRegression(random_state=0)),\n",
    "        {\"logisticregression__C\": np.logspace(-1, 1, 3)},\n",
    "    ),\n",
    "    (\n",
    "        make_pipeline(StandardScaler(), LinearSVC(random_state=0, dual=\"auto\")),\n",
    "        {\"linearsvc__C\": np.logspace(-1, 1, 3)},\n",
    "    ),\n",
    "    (\n",
    "        make_pipeline(\n",
    "            StandardScaler(),\n",
    "            KBinsDiscretizer(encode=\"onehot\", random_state=0),\n",
    "            LogisticRegression(random_state=0),\n",
    "        ),\n",
    "        {\n",
    "            \"kbinsdiscretizer__n_bins\": np.arange(5, 8),\n",
    "            \"logisticregression__C\": np.logspace(-1, 1, 3),\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        make_pipeline(\n",
    "            StandardScaler(),\n",
    "            KBinsDiscretizer(encode=\"onehot\", random_state=0),\n",
    "            LinearSVC(random_state=0, dual=\"auto\"),\n",
    "        ),\n",
    "        {\n",
    "            \"kbinsdiscretizer__n_bins\": np.arange(5, 8),\n",
    "            \"linearsvc__C\": np.logspace(-1, 1, 3),\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        make_pipeline(\n",
    "            StandardScaler(), GradientBoostingClassifier(n_estimators=5, random_state=0)\n",
    "        ),\n",
    "        {\"gradientboostingclassifier__learning_rate\": np.logspace(-2, 0, 5)},\n",
    "    ),\n",
    "    (\n",
    "        make_pipeline(StandardScaler(), SVC(random_state=0)),\n",
    "        {\"svc__C\": np.logspace(-1, 1, 3)},\n",
    "    ),\n",
    "]\n",
    "\n",
    "names = [get_name(e).replace(\"StandardScaler + \", \"\") for e, _ in classifiers]\n",
    "\n",
    "n_samples = 100\n",
    "datasets = [\n",
    "    make_moons(n_samples=n_samples, noise=0.2, random_state=0),\n",
    "    make_circles(n_samples=n_samples, noise=0.2, factor=0.5, random_state=1),\n",
    "    make_classification(\n",
    "        n_samples=n_samples,\n",
    "        n_features=2,\n",
    "        n_redundant=0,\n",
    "        n_informative=2,\n",
    "        random_state=2,\n",
    "        n_clusters_per_class=1,\n",
    "    ),\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    nrows=len(datasets), ncols=len(classifiers) + 1, figsize=(21, 9)\n",
    ")\n",
    "\n",
    "cm_piyg = plt.cm.PiYG\n",
    "\n",
    "cm_bright = ListedColormap([\"#b30065\", \"#178000\"])\n",
    "\n",
    "# iterate over datasets\n",
    "for ds_cnt, (X, y) in enumerate(datasets):\n",
    "    print(f\"\\ndataset {ds_cnt}\\n---------\")\n",
    "\n",
    "    # split into training and test part\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.5, random_state=42\n",
    "    )\n",
    "\n",
    "    # create the grid for background colors\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "    # plot the dataset first\n",
    "    ax = axes[ds_cnt, 0]\n",
    "    if ds_cnt == 0:\n",
    "        ax.set_title(\"Input data\")\n",
    "    # plot the training points\n",
    "    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright, edgecolors=\"k\")\n",
    "    # and testing points\n",
    "    ax.scatter(\n",
    "        X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6, edgecolors=\"k\"\n",
    "    )\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "\n",
    "    # iterate over classifiers\n",
    "    for est_idx, (name, (estimator, param_grid)) in enumerate(zip(names, classifiers)):\n",
    "        ax = axes[ds_cnt, est_idx + 1]\n",
    "\n",
    "        clf = GridSearchCV(estimator=estimator, param_grid=param_grid)\n",
    "        with ignore_warnings(category=ConvergenceWarning):\n",
    "            clf.fit(X_train, y_train)\n",
    "        score = clf.score(X_test, y_test)\n",
    "        print(f\"{name}: {score:.2f}\")\n",
    "\n",
    "        # plot the decision boundary. For that, we will assign a color to each\n",
    "        # point in the mesh [x_min, x_max]*[y_min, y_max].\n",
    "        if hasattr(clf, \"decision_function\"):\n",
    "            Z = clf.decision_function(np.column_stack([xx.ravel(), yy.ravel()]))\n",
    "        else:\n",
    "            Z = clf.predict_proba(np.column_stack([xx.ravel(), yy.ravel()]))[:, 1]\n",
    "\n",
    "        # put the result into a color plot\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        ax.contourf(xx, yy, Z, cmap=cm_piyg, alpha=0.8)\n",
    "\n",
    "        # plot the training points\n",
    "        ax.scatter(\n",
    "            X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright, edgecolors=\"k\"\n",
    "        )\n",
    "        # and testing points\n",
    "        ax.scatter(\n",
    "            X_test[:, 0],\n",
    "            X_test[:, 1],\n",
    "            c=y_test,\n",
    "            cmap=cm_bright,\n",
    "            edgecolors=\"k\",\n",
    "            alpha=0.6,\n",
    "        )\n",
    "        ax.set_xlim(xx.min(), xx.max())\n",
    "        ax.set_ylim(yy.min(), yy.max())\n",
    "        ax.set_xticks(())\n",
    "        ax.set_yticks(())\n",
    "\n",
    "        if ds_cnt == 0:\n",
    "            ax.set_title(name.replace(\" + \", \"\\n\"))\n",
    "        ax.text(\n",
    "            0.95,\n",
    "            0.06,\n",
    "            (f\"{score:.2f}\").lstrip(\"0\"),\n",
    "            size=15,\n",
    "            bbox=dict(boxstyle=\"round\", alpha=0.8, facecolor=\"white\"),\n",
    "            transform=ax.transAxes,\n",
    "            horizontalalignment=\"right\",\n",
    "        )\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Add suptitles above the figure\n",
    "plt.subplots_adjust(top=0.90)\n",
    "suptitles = [\n",
    "    \"Linear classifiers\",\n",
    "    \"Feature discretization and linear classifiers\",\n",
    "    \"Non-linear classifiers\",\n",
    "]\n",
    "for i, suptitle in zip([1, 3, 5], suptitles):\n",
    "    ax = axes[0, i]\n",
    "    ax.text(\n",
    "        1.05,\n",
    "        1.25,\n",
    "        suptitle,\n",
    "        transform=ax.transAxes,\n",
    "        horizontalalignment=\"center\",\n",
    "        size=\"x-large\",\n",
    "    )\n",
    "plt.savefig('Kbins_tab.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043cfbbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35633ab4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
